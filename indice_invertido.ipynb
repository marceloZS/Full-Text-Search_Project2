{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset : Fashion Product Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender masterCategory subCategory  articleType baseColour  season    year  \\\n",
       "0    Men        Apparel     Topwear       Shirts  Navy Blue    Fall  2011.0   \n",
       "1    Men        Apparel  Bottomwear        Jeans       Blue  Summer  2012.0   \n",
       "2  Women    Accessories     Watches      Watches     Silver  Winter  2016.0   \n",
       "3    Men        Apparel  Bottomwear  Track Pants      Black    Fall  2011.0   \n",
       "4    Men        Apparel     Topwear      Tshirts       Grey  Summer  2012.0   \n",
       "\n",
       "    usage                             productDisplayName  \n",
       "0  Casual               Turtle Check Men Navy Blue Shirt  \n",
       "1  Casual             Peter England Men Party Blue Jeans  \n",
       "2  Casual                       Titan Women Silver Watch  \n",
       "3  Casual  Manchester United Men Solid Black Track Pants  \n",
       "4  Casual                          Puma Men Grey T-shirt  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_data = pd.read_csv('csv/fashion_product_images.csv', delimiter=',')\n",
    "\n",
    "#Elegimos solo las columnas textuales\n",
    "fashion_data = fashion_data[['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'year', 'usage', 'productDisplayName']]\n",
    "\n",
    "fashion_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora concatenamos y creamos los txt's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Ruta de la carpeta donde se guardarán los archivos de texto\\noutput_folder = 'unprocessed_txt'\\n\\n# Crea la carpeta si no existe\\nos.makedirs(output_folder, exist_ok=True)\\n\\n# Itera a través de cada fila y guarda en un archivo de texto\\nfor index, row in fashion_data.iterrows():\\n    # Convierte la fila en una cadena, separando los elementos por un espacio\\n    row_str = ' '.join(str(item) for item in row)\\n    \\n    # Crea un archivo de texto para cada fila en la carpeta específica\\n    file_path = os.path.join(output_folder, f'doc_{index + 1}.txt')\\n    with open(file_path, 'w') as file:\\n        file.write(row_str)\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Ruta de la carpeta donde se guardarán los archivos de texto\n",
    "output_folder = 'unprocessed_txt'\n",
    "\n",
    "# Crea la carpeta si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Itera a través de cada fila y guarda en un archivo de texto\n",
    "for index, row in fashion_data.iterrows():\n",
    "    # Convierte la fila en una cadena, separando los elementos por un espacio\n",
    "    row_str = ' '.join(str(item) for item in row)\n",
    "    \n",
    "    # Crea un archivo de texto para cada fila en la carpeta específica\n",
    "    file_path = os.path.join(output_folder, f'doc_{index + 1}.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(row_str)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-procesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords #Stopwords\n",
    "from nltk.stem import SnowballStemmer #Stemming\n",
    "\n",
    "#Descargamos las stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Como todo el contenido esta en ingles, las stopwords tambien lo van a estar\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#De la misma forma como todo esta en ingles, el stemming tmb lo va a ser\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'directorio_entrada = \\'unprocessed_txt\\'\\ndirectorio_salida = \\'processed_txt\\'\\n\\n# Crea la carpeta de salida si no existe\\nos.makedirs(directorio_salida, exist_ok=True)\\n\\ncontador_docs = 0\\n\\nfor filename in os.listdir(directorio_entrada):\\n    if filename.endswith(\".txt\"):\\n\\n        contador_docs += 1\\n\\n        filepath = os.path.join(directorio_entrada, filename)\\n\\n        #Tokenizamos nuestro txt\\n        file_content = open(filepath, encoding=\"utf-8\").read().lower()\\n        tokens = nltk.word_tokenize(file_content)\\n\\n        #Filtramos para que no pertenezca a los stopwords o no sea un valor no alfanumerico (Stopwords / Valores raros)\\n        texto_filtrado = [word for word in tokens if not word in stopwords and word.isalnum()]\\n\\n        #Hacemos Stemming en el idioma respectivo\\n        texto_filtrado = [stemmer.stem(w) for w in texto_filtrado]\\n\\n        texto_procesado = \" \".join(texto_filtrado)\\n\\n        #Guardamos la lista unida en un archivo txt en el directorio de salida\\n        salida_filepath = os.path.join(directorio_salida, f\"{filename}\")\\n        \\n        #Guardamos el txt en el directorio de salida\\n        with open(salida_filepath, \\'w\\', encoding=\"utf-8\") as output_file:\\n            output_file.write(texto_procesado)\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"directorio_entrada = 'unprocessed_txt'\n",
    "directorio_salida = 'processed_txt'\n",
    "\n",
    "# Crea la carpeta de salida si no existe\n",
    "os.makedirs(directorio_salida, exist_ok=True)\n",
    "\n",
    "contador_docs = 0\n",
    "\n",
    "for filename in os.listdir(directorio_entrada):\n",
    "    if filename.endswith(\".txt\"):\n",
    "\n",
    "        contador_docs += 1\n",
    "\n",
    "        filepath = os.path.join(directorio_entrada, filename)\n",
    "\n",
    "        #Tokenizamos nuestro txt\n",
    "        file_content = open(filepath, encoding=\"utf-8\").read().lower()\n",
    "        tokens = nltk.word_tokenize(file_content)\n",
    "\n",
    "        #Filtramos para que no pertenezca a los stopwords o no sea un valor no alfanumerico (Stopwords / Valores raros)\n",
    "        texto_filtrado = [word for word in tokens if not word in stopwords and word.isalnum()]\n",
    "\n",
    "        #Hacemos Stemming en el idioma respectivo\n",
    "        texto_filtrado = [stemmer.stem(w) for w in texto_filtrado]\n",
    "\n",
    "        texto_procesado = \" \".join(texto_filtrado)\n",
    "\n",
    "        #Guardamos la lista unida en un archivo txt en el directorio de salida\n",
    "        salida_filepath = os.path.join(directorio_salida, f\"{filename}\")\n",
    "        \n",
    "        #Guardamos el txt en el directorio de salida\n",
    "        with open(salida_filepath, 'w', encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(texto_procesado)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Indice Invertido en Memoria Secundaria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asocio cada termino a un ID de tamaño fijo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import re\n",
    "import collections\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def sort_terms(term_postings_list):\n",
    "    \"\"\" Sorts dictionary terms in alphabetical order \"\"\"\n",
    "    print(\" -- Sorting terms...\")\n",
    "    sorted_dictionary = OrderedDict() # keep track of insertion order\n",
    "    sorted_terms = sorted(term_postings_list)\n",
    "    for term in sorted_terms:\n",
    "        result = [int(docIds) for docIds in term_postings_list[term]]\n",
    "        result_tftd = calculate_tftd(result)\n",
    "        sorted_dictionary[term] = result_tftd\n",
    "    return sorted_dictionary\n",
    "\n",
    "def calculate_tftd(pl_with_duplicates):\n",
    "    \"\"\" Add term frequency of term in each document \"\"\"\n",
    "    # print(pl_with_duplicates)\n",
    "    counter = collections.Counter(pl_with_duplicates)\n",
    "    pl_tftd = [[int(docId), counter[docId]] for docId in counter.keys()]\n",
    "    return pl_tftd\n",
    "\n",
    "def write_block_to_disk(term_postings_list, block_number):\n",
    "    \"\"\" Writes index of the block (dictionary + postings list) to disk \"\"\"\n",
    "    # Define block\n",
    "    base_path = 'index_blocks/'\n",
    "    block_name = 'block-' + str(block_number) + '.txt'\n",
    "    block = open(base_path + block_name, 'a+')\n",
    "    print(\" -- Writing term-positing list block: \" + block_name + \"...\")\n",
    "    # Write term : posting lists to block\n",
    "    for index, term in enumerate(term_postings_list):\n",
    "        # Term - Posting List Format\n",
    "        # term:[docID1, docID2, docID3]\n",
    "        # e.g. cat:[4,9,21,42]\n",
    "        block.write(str(term) + \":\" + str((term_postings_list[term])) + \"\\n\")\n",
    "    block.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08e3c4c91ca5c58e479ea699df7b1a7e3743831ac17b27e1e967c7d74f41df23\n",
      "646b1045ad1e525978a10d5a7b55f93440b821e514cd98fc43f4d2d1ccc11b20\n",
      "511a79071666e78fa9283cc066ebafd2fd5e45e7f49705a34b4cdf023d818e67\n",
      "33a0d09c376fe8b083b04d0a974d2f8577242ed703f66d5d7edac3541bf0826c\n",
      "df108922e732f36d1805f839880815580356669b8d1ff8cd94bcb1ca34e177e1\n",
      "16477688c0e00699c6cfa4497a3612d7e83c532062b64b250fed8908128ed548\n",
      "2dc46af1b78c32dfe99dedfbdff6ca7f33e0f652716482f55b03622b67c06dea\n",
      "c633eb52f63cec1d6b02d42448254cd801f5cbcaa254313c2795ce38bcd3a7fa\n",
      "74dd730b5c36c1cb4fdbb4bce1764c57f260190f94ba52d877308332f1dfe363\n",
      "20f65c28671b40937c5bf23acc7c6f37e5a5ec0622e347b57685725df5ba9e50\n",
      "08e3c4c91ca5c58e479ea699df7b1a7e3743831ac17b27e1e967c7d74f41df23\n",
      "df108922e732f36d1805f839880815580356669b8d1ff8cd94bcb1ca34e177e1\n",
      "16477688c0e00699c6cfa4497a3612d7e83c532062b64b250fed8908128ed548\n",
      "a52514252adf86066966790e947f72ef6ea6690a6852d5bb5480ee2eeb663395\n"
     ]
    }
   ],
   "source": [
    "def spimi_invert(documents, block_size_limit):\n",
    "    \"\"\" Applies the Single-pass in-memory indexing algorithm \"\"\"\n",
    "    block_number = 0\n",
    "    documents_count = len(documents)\n",
    "    dictionary = {} # (term - postings list)\n",
    "    for index, docID in enumerate(documents):\n",
    "        for term in documents[docID]:\n",
    "            # If term occurs for the first time\n",
    "            if term not in dictionary:\n",
    "                # Add term to dictionary, create new postings list, and add docID\n",
    "                dictionary[term] = [docID]\n",
    "            # else:\n",
    "            #     # If term has a subsequent occurence\n",
    "            #     if docID not in dictionary[term]:\n",
    "            #         # Add a posting (docID) to the existing posting list of the term\n",
    "            #         dictionary[term].append(docID)\n",
    "            else:\n",
    "                dictionary[term].append(docID)\n",
    "        if sys.getsizeof(dictionary) > block_size_limit or (index == documents_count-1):\n",
    "            temp_dict = sort_terms(dictionary)\n",
    "            write_block_to_disk(temp_dict, block_number)\n",
    "            temp_dict = {}\n",
    "            block_number += 1\n",
    "            dictionary = {}\n",
    "    print(\"SPIMI invert complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
