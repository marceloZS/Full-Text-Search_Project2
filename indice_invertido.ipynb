{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset : Fashion Product Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender masterCategory subCategory  articleType baseColour  season    year  \\\n",
       "0    Men        Apparel     Topwear       Shirts  Navy Blue    Fall  2011.0   \n",
       "1    Men        Apparel  Bottomwear        Jeans       Blue  Summer  2012.0   \n",
       "2  Women    Accessories     Watches      Watches     Silver  Winter  2016.0   \n",
       "3    Men        Apparel  Bottomwear  Track Pants      Black    Fall  2011.0   \n",
       "4    Men        Apparel     Topwear      Tshirts       Grey  Summer  2012.0   \n",
       "\n",
       "    usage                             productDisplayName  \n",
       "0  Casual               Turtle Check Men Navy Blue Shirt  \n",
       "1  Casual             Peter England Men Party Blue Jeans  \n",
       "2  Casual                       Titan Women Silver Watch  \n",
       "3  Casual  Manchester United Men Solid Black Track Pants  \n",
       "4  Casual                          Puma Men Grey T-shirt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_data = pd.read_csv('csv/fashion_product_images.csv', delimiter=',')\n",
    "\n",
    "#Elegimos solo las columnas textuales\n",
    "fashion_data = fashion_data[['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'year', 'usage', 'productDisplayName']]\n",
    "\n",
    "fashion_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora concatenamos y creamos los txt's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta donde se guardarán los archivos de texto\n",
    "output_folder = 'unprocessed_txt'\n",
    "\n",
    "# Crea la carpeta si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Itera a través de cada fila y guarda en un archivo de texto\n",
    "for index, row in fashion_data.iterrows():\n",
    "    # Convierte la fila en una cadena, separando los elementos por un espacio\n",
    "    row_str = ' '.join(str(item) for item in row)\n",
    "    \n",
    "    # Crea un archivo de texto para cada fila en la carpeta específica\n",
    "    file_path = os.path.join(output_folder, f'doc{index + 1}.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-procesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import nltk.downloader\n",
    "nltk.downloader.download('punkt')\n",
    "from nltk.corpus import stopwords #Stopwords\n",
    "from nltk.stem import SnowballStemmer #Stemming\n",
    "import re\n",
    "\n",
    "\n",
    "#Descargamos las stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Como todo el contenido esta en ingles, las stopwords tambien lo van a estar\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#De la misma forma como todo esta en ingles, el stemming tmb lo va a ser\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_entrada = 'unprocessed_txt'\n",
    "directorio_salida = 'processed_txt'\n",
    "\n",
    "# Crea la carpeta de salida si no existe\n",
    "os.makedirs(directorio_salida, exist_ok=True)\n",
    "\n",
    "contador_docs = 0\n",
    "\n",
    "for filename in os.listdir(directorio_entrada):\n",
    "    if filename.endswith(\".txt\"):\n",
    "\n",
    "        contador_docs += 1\n",
    "\n",
    "        filepath = os.path.join(directorio_entrada, filename)\n",
    "\n",
    "        #Tokenizamos nuestro txt\n",
    "        file_content = open(filepath, encoding=\"utf-8\").read().lower()\n",
    "        tokens = nltk.word_tokenize(file_content)\n",
    "\n",
    "        #Filtramos para que no pertenezca a los stopwords o no sea un valor no alfanumerico (Stopwords / Valores raros)\n",
    "        texto_filtrado = [word for word in tokens if not word in stopwords and re.match(\"^[a-zA-Z]+$\", word)]\n",
    "        #texto_filtrado = [word for word in tokens if not word in stopwords and word.isalnum()]\n",
    "\n",
    "        #Hacemos Stemming en el idioma respectivo\n",
    "        texto_filtrado = [stemmer.stem(w) for w in texto_filtrado]\n",
    "\n",
    "        texto_procesado = \" \".join(texto_filtrado)\n",
    "\n",
    "        #Guardamos la lista unida en un archivo txt en el directorio de salida\n",
    "        salida_filepath = os.path.join(directorio_salida, f\"{filename}\")\n",
    "        \n",
    "        #Guardamos el txt en el directorio de salida\n",
    "        with open(salida_filepath, 'w', encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(texto_procesado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Indice Invertido en Memoria Secundaria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cómo funciona la creacion de los bloques:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import os\n",
    "import heapq\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Metodos auxiliares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_terms(term_postings_list):\n",
    "    \"\"\" Sorts dictionary terms in alphabetical order \"\"\"\n",
    "    print(\" -- Sorting terms...\")\n",
    "    sorted_dictionary = {} # keep track of insertion order\n",
    "    sorted_terms = sorted(term_postings_list)\n",
    "    for term in sorted_terms:\n",
    "        result = [docIds for docIds in term_postings_list[term]]\n",
    "        result_tftd = calculate_tftd(result)\n",
    "        sorted_dictionary[term] = result_tftd\n",
    "    return sorted_dictionary\n",
    "\n",
    "def calculate_tftd(pl_with_duplicates):\n",
    "    \"\"\" Add term frequency of term in each document \"\"\"\n",
    "    # print(pl_with_duplicates)\n",
    "    counter = collections.Counter(pl_with_duplicates)\n",
    "    pl_tftd = [[docId, counter[docId]] for docId in counter.keys()]\n",
    "    return pl_tftd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_block_to_disk(term_postings_list, block_number):\n",
    "    \"\"\" Writes index of the block (dictionary + postings list) to disk \"\"\"\n",
    "\n",
    "    # Crea la carpeta de salida si no existe\n",
    "    os.makedirs('index_blocks', exist_ok=True)\n",
    "\n",
    "    # Define block\n",
    "    base_path = 'index_blocks/'\n",
    "    block_name = 'block-' + str(block_number) + '.txt'\n",
    "    block = open(base_path + block_name, 'a+')\n",
    "    print(\" -- Writing term-positing list block: \" + block_name + \"...\")\n",
    "    # Write term : posting lists to block\n",
    "    for index, term in enumerate(term_postings_list):\n",
    "        # Term - Posting List Format\n",
    "        # term:[docID1, docID2, docID3]\n",
    "        # e.g. cat:[4,9,21,42]\n",
    "        block.write(str(term) + \":\" + str((term_postings_list[term])) + \"\\n\")\n",
    "    block.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creacion de los bloques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE BLOCKS\n",
    "\n",
    "def merge_blocks(block_folder, output_file):\n",
    "    block_files = [os.path.join(block_folder, block) for block in os.listdir(block_folder)]\n",
    "    block_files.sort()  \n",
    "    block_handles = [open(file, 'r', encoding='utf-8') for file in block_files] \n",
    "    merge_heap = []  \n",
    "    term_postings_dict = {}\n",
    "\n",
    "    for i, block_handle in enumerate(block_handles):\n",
    "        line = block_handle.readline().strip()\n",
    "        if line:\n",
    "            term, postings = line.split(':', 1)\n",
    "            postings = eval(postings) \n",
    "            term_postings_dict[term] = postings\n",
    "            heapq.heappush(merge_heap, (term, i))\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_handle:\n",
    "        while merge_heap:\n",
    "            min_term, block_index = heapq.heappop(merge_heap)\n",
    "\n",
    "            output_handle.write(f\"{min_term}:{str(term_postings_dict[min_term])}\\n\")\n",
    "\n",
    "            line = block_handles[block_index].readline().strip()\n",
    "            if line:\n",
    "                term, postings = line.split(':', 1)\n",
    "                postings = eval(postings)  \n",
    "                term_postings_dict[term] = postings\n",
    "                heapq.heappush(merge_heap, (term, block_index))\n",
    "    for block_handle in block_handles:\n",
    "        block_handle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-0.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-1.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-2.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-3.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-4.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-5.txt...\n",
      "BLOCKS creation complete!\n"
     ]
    }
   ],
   "source": [
    "# SPIMI\n",
    "\n",
    "def spimi_invert(directory, block_size_limit, output_folder):\n",
    "    documents = os.listdir(directory)\n",
    "    documents_count = len(documents)\n",
    "    documents_counter = 0\n",
    "    block_number = 0\n",
    "    dictionary = {}\n",
    "\n",
    "    for docID in documents:\n",
    "        if docID.endswith(\".txt\"):\n",
    "\n",
    "            documents_counter += 1\n",
    "\n",
    "            file_route = os.path.join(directory, docID)\n",
    "\n",
    "            # Tokenizamos nuestro txt\n",
    "            file_content = open(file_route, encoding=\"utf-8\").read().lower()\n",
    "            terms = file_content.split()\n",
    "\n",
    "            for term in terms:\n",
    "                if term not in dictionary:\n",
    "            \n",
    "                    dictionary[term] = [docID]\n",
    "                else:\n",
    "                    dictionary[term].append(docID)\n",
    "\n",
    "            if sys.getsizeof(dictionary) > block_size_limit or (documents_counter == documents_count - 1):\n",
    "                temp_dict = sort_terms(dictionary)\n",
    "                write_block_to_disk(temp_dict, block_number)\n",
    "                temp_dict = {}\n",
    "                block_number += 1\n",
    "                dictionary = {}\n",
    "\n",
    "    print(\"BLOCKS creation complete!\")\n",
    "    output_file = os.path.join(output_folder, 'inverted_index.txt')\n",
    "    merge_blocks('index_blocks', output_file)\n",
    "\n",
    "block_size_limit = 900 * 100\n",
    "output_folder = 'output_folder'  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "spimi_invert('processed_txt', block_size_limit, output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY_SEARCH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-0.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-1.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-2.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-3.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-4.txt...\n",
      " -- Sorting terms...\n",
      " -- Writing term-positing list block: block-5.txt...\n",
      "BLOCKS creation complete!\n"
     ]
    }
   ],
   "source": [
    "block_size_limit = 900*100\n",
    "\n",
    "# Llama a la función spimi_invert con la dirección del directorio y el límite del tamaño del diccionario\n",
    "spimi_invert('processed_txt', block_size_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
